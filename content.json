{"meta":{"title":"Welcome to Hongyi LI's website","subtitle":null,"description":"Find more about HYLI","author":"HongYi LI","url":"http://yoursite.com","root":"/"},"pages":[{"title":"Notes","date":"2020-05-18T14:51:24.000Z","updated":"2020-06-02T10:11:14.959Z","comments":true,"path":"Notes/index.html","permalink":"http://yoursite.com/Notes/index.html","excerpt":"","text":"Linear Algebra (MIT 18.06) (The linear algebra notes are based on MIT OCW 18.06 Lecture Notes) Linear Programming (from Prof. Shu-Cheng Fang)"},{"title":"Project","date":"2020-05-15T07:11:24.000Z","updated":"2020-05-15T07:18:39.819Z","comments":true,"path":"Project/index.html","permalink":"http://yoursite.com/Project/index.html","excerpt":"","text":"Waiting for update."},{"title":"Feiyue","date":"2020-05-15T06:16:39.000Z","updated":"2020-05-27T08:03:17.018Z","comments":true,"path":"Feiyue/index.html","permalink":"http://yoursite.com/Feiyue/index.html","excerpt":"","text":"点击下载《华中大电气飞跃手册》或访问Github repo获取 飞跃手册是什么 《华中大电气飞跃手册》是一本面向电气学子的申请指南，由华中科技大学电气学院九位16级本科生在学院的领导与支持下编写而成。总结了编委会成员的留学申请经验，涵盖关于留学申请的常见问题，包含道路选择、托福雅思GRE考试、科研暑研、套瓷、签证等内容。意图帮助大家对留学申请全过程建立起比较清晰、全面的认识。除此之外，编委会还整理了学校海外交流项目政策并汇编了电气学院往届学长学姐的留学申请经历，帮助同学们更好地明确自己的定位，更加合理地进行学校选择和申请准备。 我为什么要做这件事 在我们之前，电气学院的编写经验为零，这给了我们很大的挑战，也给了我们很大的发挥空间。非常感谢我们的特邀编写顾问，光电学院的两位前主编，无私地与我们分享了他们在飞跃手册编写过程中积累的经验和踩过的坑，帮我们制订了编写计划的雏形。我想这就是飞跃手册的精神内核，一种甘于分享、不计回报的传承精神，也是我们希望通过这本华中大电气飞跃手册传达给大家的一种精神、一种期望。我们期待着这本手册能给接下来每一届电气学子的留学申请带来帮助，也期待着大家申请之后可以回过头来，不断地为飞跃手册增补内容，亦或是推翻重写，让它成为与时俱进、代代传承的一本手册。化用我高中的校训，“传承是飞跃手册强大的理由”。 留学申请的历程是漫长而又枯燥的，就像是reinforcement learning的环境中采用了非常sparse的reward，但人与机器的区别就在于，就算很长时间没有得到reward或者penalty，人也可以采取各种行为去学习。编写飞跃手册的初衷，其实就是希望帮助大家在这样一个sparse的环境里，快速找到reward，顺利地走下去。但如果你迟迟没有找到reward，也不要急躁，要相信“心诚求之，虽不中不远矣”，保持积极的心态去努力，相信你一定能有所收获。 感谢所有支持华中大电气飞跃手册编写的老师和同学，衷心祝愿所有电气学子都能在追逐梦想的道路上肆意奔跑。 编委会名单 主编： 刘剑涛 李弘毅 彭成志 编委： 张雅萱 李 越 黄子安 罗 斌 顾天存 李玥廷 特邀编写顾问： 刘明辰 陈雄超 版权声明 本手册旨在为相关专业学子提供留学参考信息，手册版权归华中科技大学电气与电子工程学院和编辑委员会所有，拥有编辑、修改、复制和发行的权利，并授权华中科技大学电气与电子工程学院学业指导中心全部权利。您仅可以在个人非商业用途时，使用手册中的相关内容，使用时必须注明“来源：《华中大电气飞跃手册2020》”。 未经允许，任何组织和个人不得以任何形式、通过任何渠道编辑、修改、复制和发行本手册的全部或部分内容，不得将本手册用于商业用途，不得进行任何违反版权条例的行为。在出现版权纠纷时，编辑委员会授权华中科技大学电气与电子工程学院学业指导中心全权代理处理。 由于编委会水平有限，本手册难免存在错误和不足之处，如果您对本手册编写存在疑问，或有修改意见和建议，欢迎您联系：027-87542364，或通过学业指导中心官方微信公众号“HUST明德学堂”与我们联系。 友情链接 华中科技大学 华中大电气学院 华中科技大学OEI&amp;SES飞跃手册"},{"title":"Research","date":"2020-05-15T06:07:19.000Z","updated":"2020-05-15T07:18:52.440Z","comments":true,"path":"Research/index.html","permalink":"http://yoursite.com/Research/index.html","excerpt":"","text":"Research Interest Smart Power Control Theory Convex Optimization Reinforcement Learning"},{"title":"Album","date":"2020-05-15T06:07:10.000Z","updated":"2020-05-23T15:18:01.528Z","comments":true,"path":"Album/index.html","permalink":"http://yoursite.com/Album/index.html","excerpt":"","text":"We won the first prize in “Challenge Cup” National Undergraduate Extra-Curricular Academic Science and Technology Works Competition, Nov. 2019, BUAA. From left to right: Jiarui HAO, Yubin HUANG, Wenli XU, Hongyi LI, Lei ZHOU, Jinyu WEN, Qing CHEN, Jun LUO, Haonan LEI, Buyang DU. I was studying in C12. Me with Yue HU and Jiaqi WANG. They taught me a lot when I was in the SEEE Student Union. When I was a freshman in HUST."},{"title":"About","date":"2020-05-15T06:06:05.000Z","updated":"2020-06-30T05:50:56.978Z","comments":true,"path":"About/index.html","permalink":"http://yoursite.com/About/index.html","excerpt":"","text":"Education B.E. in Electrical and Electronic Engineering, Huazhong University of Science and Technology, Sept. 2016-Jun. 2020 Overall GPA: 3.91/4 IELTS: 7.5 (L:8.5 R:8.5 W:6.5 S:6.5) Relevant Coursework: Electric Driving Control System 96, Design of Electromagnetic Device 98, Principles of Microcontroller and Applications 95, Digital Electronics 97, Analog Electronic Technology 97, Power Electronics Equipment and System 94, Probability Theory and Mathematical Statistics 96 Competition Experience First Prize, “Challenge Cup” National Undergraduate Extra-Curricular Academic Science and Technology Works, Nov. 2019 Third-Grade Award, “Chinese Society for Electrical Engineering Cup” Electrical Mathematics Modeling Competition, May 2019 Third-Grade Award, Mathematics Modeling Invitational for Students in Central China, May 2019 Honors &amp; Awards Bachelor’s Honors Degree, School of Electrical and Electronic Engineering, Huazhong University of Science and Technology, 2020 Outstanding Graduate of Huazhong University of Science and Technology, 2020 Outstanding Individual in University Students Innovation and Entrepreneurship Projocet, Huazhong University of Science and Technology, 2018 and 2019, respectively Outstanding Communist, Huazhong University of Science and Technology, 2019 “Outstanding Student Cadres” of Huazhong University of Science and Technology, 2017-2018 “Outstanding Cadres of Communist Youth League” of Huazhong University of Science and Technology, 2017-2018 and 2018-2019, respectively “Outstanding Members of Communist Youth League” of Huazhong University of Science and Technology, 2016-2017 Tellhow Scholarship, TELLHOW SCI-TECH CO., LTD, 2019 Grant of Phoenix Contact, 2019 Siyuan Scholarship, 2018 Self-improvement Scholarship, School of Electrical and Electronic Engineering, Huazhong University of Science and Technology, 2017 Languages Chinese: Native English: Fluent Contonese: Fluent Portugese: Um pouco"},{"title":"Linear Algebra","date":"2020-06-02T09:49:21.000Z","updated":"2020-06-02T09:51:17.076Z","comments":true,"path":"Notes/Linear-Algebra/index.html","permalink":"http://yoursite.com/Notes/Linear-Algebra/index.html","excerpt":"","text":"Lecture 01：The Geometry of Linear Equations Row Picture Column Picture Matrix Picture Matrix Multiplication Linear Independence Lecture 01：The Geometry of Linear Equations The fundamental problem of linear algebra is to solve n linear equations in n unknowns, for example: 2x−y = 0−x+2y = 3\\begin{aligned} 2x-y\\,=\\,0\\\\ -x+2y\\,=\\,3 \\end{aligned} 2x−y=0−x+2y=3​ In the first lecture, Dr. Strang show us three ways to view this problem. Row Picture Plot the points that satisfied each equation. The intersection of the plots (if they do intersect) is the solution of the system of above equations, which is x = 1,y = 2x\\,=\\,1, y\\,=\\,2x=1,y=2. Figure 1: The plots of the above equations intersect at the point (1,2) We substitute (1,2) into the original system of equations to check it’s validity: 2⋅1−2 = 0−1+2⋅2 = 3\\begin{aligned} 2\\cdot1-2\\,=\\,0\\\\ -1+2\\cdot2\\,=\\,3 \\end{aligned} 2⋅1−2=0−1+2⋅2=3​ Similarly, the solution of a three dimensional system is the common intersection of those three planes (if there does exist one). Column Picture In the column picture, we rewrite the system as a single equation by turning the coefficients in the column of the system into vectors: x[21]+y[−12]=[03]\\begin{aligned} x \\left[ \\begin{array}{c} 2\\\\ 1 \\end{array} \\right] +y \\left[ \\begin{array}{c} -1\\\\ 2 \\end{array} \\right] = \\left[ \\begin{array}{c} 0\\\\ 3 \\end{array} \\right] \\end{aligned} x[21​]+y[−12​]=[03​]​ Given two vectors c and d and scalars x and y, the sum xc+yd is called a linear combination of c and d, which is an important concept throughout Linear Algebra. Geometrically, we are looking for a pair of x and y which satisfies that x copies of vector [2−1]\\left[\\begin{array}{c}2\\\\-1\\end{array}\\right][2−1​] added to y copies of vector [−12]\\left[\\begin{array}{c}-1\\\\2\\end{array}\\right][−12​] equals the vector [03]\\left[\\begin{array}{c}0\\\\3\\end{array}\\right][03​]. As shown in Figure 2, x=1 and y=2x=1 \\;\\text{and}\\; y=2x=1andy=2 agreeing with the result we got from row picture. Figure 2: A linear combination of the column vectors. In the three dimensions, the column picture requires to find a linear combination of 3-dimensional vectors that equals to the vector b. Matrix Picture Rewrite the system of equations as a single equation by using matrices and vectors: [2−1−12][xy]=[03]\\begin{aligned} \\left[ \\begin{array}{cc} 2 &amp; -1\\\\ -1 &amp; 2\\\\ \\end{array} \\right] \\left[ \\begin{array}{c} x\\\\ y \\end{array} \\right] =\\left[ \\begin{array}{c} 0\\\\ 3 \\end{array} \\right] \\end{aligned} [2−1​−12​][xy​]=[03​]​ The matrix [2−1−12]\\left[\\begin{array}{cc}2&amp;-1\\\\-1&amp;2\\end{array}\\right][2−1​−12​] is called the coefficient matrix. The vector x=[xy]\\bold{x}=\\left[\\begin{array}{c}x\\\\y\\end{array}\\right]x=[xy​] is the vector of unknowns. The value on the right hand side of the equations form the vector b: Ax=bA\\bold{x}=\\bold{b} Ax=b The three dimensional matrix picture is similar to the two dimensional one, except that the vectors and matrices increase in size. Matrix Multiplication How do we multiply a matrix A by a vector x? [2513][12]= ?\\left[ \\begin{array}{cc} 2&amp;5\\\\ 1&amp;3 \\end{array} \\right] \\left[ \\begin{array}{c} 1\\\\2 \\end{array} \\right] =\\;? [21​53​][12​]=? The method Dr. Strang suggests is to think of the entries of x as the coefficients of a linear combination of the column vectors of the matrix: [2513][12]=1[21]+2[53]=[127]\\left[ \\begin{array}{cc} 2&amp;5\\\\1&amp;3 \\end{array} \\right] \\left[ \\begin{array}{c} 1\\\\2 \\end{array} \\right] =1\\left[ \\begin{array}{c} 2\\\\1 \\end{array} \\right] +2\\left[ \\begin{array}{c} 5\\\\3 \\end{array} \\right] =\\left[ \\begin{array}{c} 12\\\\7 \\end{array} \\right] [21​53​][12​]=1[21​]+2[53​]=[127​] The technique shows that Ax is a linear combination of the columns of A. Also, you can calculate the product Ax by taking dot product of each row of A with the vector x: [2513][12]=[2⋅1+5⋅21⋅1+3⋅2]=[127]\\left[ \\begin{array}{cc} 2&amp;5\\\\1&amp;3 \\end{array} \\right] \\left[ \\begin{array}{c} 1\\\\2 \\end{array} \\right] =\\left[ \\begin{array}{c} 2\\cdot1+5\\cdot2\\\\1\\cdot1+3\\cdot2 \\end{array} \\right] =\\left[ \\begin{array}{c} 12\\\\7 \\end{array} \\right] [21​53​][12​]=[2⋅1+5⋅21⋅1+3⋅2​]=[127​] Linear Independence In the column and matrix pictures, the right hand side of the equation is a vector b. Given a matrix A, if we can solve: Ax=bA\\bold{x}=\\bold{b} Ax=b for every possible b, we say that A is an inversible matrix, which means that the linear combinations of the column vectors fill the xy-plane (in two dimensional case). Otherwise, we say that A is a singular matrix, whose column vectors are linear dependent, or in other words, all linear combinations of those vectors lie on a point or line (in two dimensional case). In such case, the combinations don’t fill the whole space."},{"title":"Linear Programming","date":"2020-06-02T02:15:37.000Z","updated":"2020-07-01T08:12:45.447Z","comments":true,"path":"Notes/Linear-Programming/index.html","permalink":"http://yoursite.com/Notes/Linear-Programming/index.html","excerpt":"","text":"I am learning the book “Optimization Models” recently and I see a blogger recommends Prof. Shu-Cheng Fang’s course “Linear Programming” as a supplement. The OCW course video is found in Bilibili. Lecture 0 - Lecture 2: Introduction, Perliminaries Standard Form of LP Model Explicit Form Matrix Form Embedded Assumptions in LP Converting to Standard LP Rule 1: Unrestricted (free) variables Rule 2: Inequality constraints Rule 3: Minimization of the Objective Funtion Potential Problem caused by Standardizing Lecture 3: Geometry of LP Terminologies Baseline model: Feasible domain Feasible solution Consistency Background knowledge Definition of hyperplane Properties of hyperplanes Properties of feasible solution set Properties of optimal solutions Graphic method Step 1: Step 2: Fundamental theorem of LP Background knowledge The geometric meaning of the feasible domain Interior and boundary points How to define a convex set S Difference among boundary points Finding extreme points Managing extreme points algebraically What do extreme points bring us? Resolution theorem Fundamental theorem of LP Lecture 4: Simplex Method Basic Idea: Nondegeneracy Property 1: Property 2: Simplex method under nondegeneracy Basic idea: Definition Lecture 0 - Lecture 2: Introduction, Perliminaries In linear programming (LP), we established a LP model to solve a realistic problem (in approximate way). To establish the LP model, the steps is as followed: What are the variables to be involved? (This is the first thing we should think about) What’s the objective function? How are the variables constrained? (The order of step 2 and 3 is not fixed, since they are independent) Standard Form of LP Model Including: n variables 1 objective function m constraints non-negative variables Explicit Form Minimize z=c1x1+c2x2+⋯+cnxnsubject toa11x1+a12x2+⋯+a1nxn=b1a21x1+a22x2+⋯+a2nxn=b2⋮am1x1+am2x2+⋯+amnxn=bmx1≥0, x2≥0, ,⋯ , xn≥0\\begin{aligned} &amp; \\text{Minimize} &amp;\\;\\bold{z}=c_1x_1+c_2x_2+\\cdots+c_nx_n\\\\ &amp; \\text{subject to} &amp;\\\\ &amp; &amp; a_{11}x_1+a_{12}x_2+\\cdots+a_{1n}x_n=b_1\\\\ &amp; &amp; a_{21}x_1+a_{22}x_2+\\cdots+a_{2n}x_n=b_2\\\\ &amp; &amp; \\vdots\\\\ &amp; &amp; a_{m1}x_1+a_{m2}x_2+\\cdots+a_{mn}x_n=b_m\\\\ &amp; &amp; x_1\\geq0,\\;x_2\\geq0,\\;,\\cdots,\\;x_n\\geq0 \\end{aligned} ​Minimizesubject to​z=c1​x1​+c2​x2​+⋯+cn​xn​a11​x1​+a12​x2​+⋯+a1n​xn​=b1​a21​x1​+a22​x2​+⋯+a2n​xn​=b2​⋮am1​x1​+am2​x2​+⋯+amn​xn​=bm​x1​≥0,x2​≥0,,⋯,xn​≥0​ Minimizing one objective function. Equality constrains. Non-negative variables. Matrix Form Min cTxs.t. AX=bx≥0\\begin{aligned} &amp;&amp; \\text{Min} \\; \\bold{c}^T\\bold{x}\\\\ &amp;&amp; \\text{s.t.} \\; \\bold{AX}=\\bold{b}\\\\ &amp;&amp; \\bold{x}\\geq0 \\end{aligned} ​​MincTxs.t.AX=bx≥0​ in which, c=[c1c2⋮cn]\\bold{c}=\\left[\\begin{array}{c}c_1\\\\c_2\\\\\\vdots\\\\c_n\\end{array}\\right]c=⎣⎢⎢⎢⎡​c1​c2​⋮cn​​⎦⎥⎥⎥⎤​ is the cost vector, x=[x1x2⋮xn]\\bold{x}=\\left[\\begin{array}{c}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{array}\\right]x=⎣⎢⎢⎢⎡​x1​x2​⋮xn​​⎦⎥⎥⎥⎤​ is the solution vector, b=[b1b2⋮bn]\\bold{b}=\\left[\\begin{array}{c}b_1\\\\b_2\\\\\\vdots\\\\b_n\\end{array}\\right]b=⎣⎢⎢⎢⎡​b1​b2​⋮bn​​⎦⎥⎥⎥⎤​ is the right-hand-side vector and A=[a11a12⋯a1na21a22⋯a12n⋮⋮⋱⋮an1an2⋯ann]\\bold{A}=\\left[\\begin{array}{cccc}a_{11}&amp;a_{12}&amp;\\cdots&amp;a_{1n}\\\\a_{21}&amp;a_{22}&amp;\\cdots&amp;a_{12n}\\\\\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\a_{n1}&amp;a_{n2}&amp;\\cdots&amp;a_{nn}\\\\\\end{array}\\right]A=⎣⎢⎢⎢⎡​a11​a21​⋮an1​​a12​a22​⋮an2​​⋯⋯⋱⋯​a1n​a12n​⋮ann​​⎦⎥⎥⎥⎤​is the constrain matrix. Embedded Assumptions in LP Proportionality Assumption No discount No economy of return to scale Additivity Assumption Total contribution = Sum of contributions of individual variables Divisibility Assumption Any fractional value is allowed Certainty Assumption Each parameter is know for sure Converting to Standard LP Rule 1: Unrestricted (free) variables For any xi∈Rx_i\\in\\bold{R}xi​∈R, we can divide it into a positive component xi+x_i^+xi+​ and a negative component xi−x_i^-xi−​, which satisfy xi+={xi+, if xi≥00, otherwisex_i^+= \\left\\{ \\begin{aligned} &amp;x_i^+,&amp;\\;\\text{if}\\;x_i\\geq0\\\\ &amp;0,&amp;\\;\\text{otherwise} \\end{aligned} \\right. xi+​={​xi+​,0,​ifxi​≥0otherwise​ xi−={0, if xi≥0xi−, otherwisex_i^-= \\left\\{ \\begin{aligned} &amp;0,&amp;\\;\\text{if}\\;x_i\\geq0\\\\ &amp;x_i^-,&amp;\\;\\text{otherwise} \\end{aligned} \\right. xi−​={​0,xi−​,​ifxi​≥0otherwise​ Therefore, xix_ixi​ can be written as xi=xi++xi−x_i=x_i^++x_i^-xi​=xi+​+xi−​, in which xi+, xi−≥0x_i^+,\\;x_i^-\\geq0xi+​,xi−​≥0. If we have an absolute valve ∣xi∣|x_i|∣xi​∣ in the LP model, it can be displaced by ∣xi∣=xi++xi−|x_i|=x_i^++x_i^-∣xi​∣=xi+​+xi−​. Notice: Such decomposition introduces an extra constraint: xi+×xi−=0x_i^+\\times x_i^-=0xi+​×xi−​=0, which is important by usually ignored. This second-ordered constraint makes sure the uniqueness of the solution. Rule 2: Inequality constraints For a11x1+a12x2+⋯+a1nxn≤b1a_{11}x_1+a_{12}x_2+\\cdots+a_{1n}x_n\\leq b_1 a11​x1​+a12​x2​+⋯+a1n​xn​≤b1​ we introduce a slack variable sis_isi​ and the original inequality constraint can be transformed to a11x1+a12x2+⋯+a1nxn+si=b1a_{11}x_1+a_{12}x_2+\\cdots+a_{1n}x_n+s_i=b_1 a11​x1​+a12​x2​+⋯+a1n​xn​+si​=b1​ in which si≥0s_i\\geq0si​≥0. Similarly, for a11x1+a12x2+⋯+a1nxn≥b1a_{11}x_1+a_{12}x_2+\\cdots+a_{1n}x_n\\geq b_1 a11​x1​+a12​x2​+⋯+a1n​xn​≥b1​ we introduce an excess variable eie_iei​ and the original inequality constraint can be transformed to a11x1+a12x2+⋯+a1nxn−ei=b1a_{11}x_1+a_{12}x_2+\\cdots+a_{1n}x_n-e_i=b_1 a11​x1​+a12​x2​+⋯+a1n​xn​−ei​=b1​ in which ei≥0e_i\\geq0ei​≥0. Rule 3: Minimization of the Objective Funtion Max cT=−Min(−cTx)\\boldsymbol{Max\\;c^T=-Min(-c^Tx)} MaxcT=−Min(−cTx) Potential Problem caused by Standardizing One quadratic constraint xi+×xi−=0x_i^+\\times x_i^-=0xi+​×xi−​=0 is missing in the LP model Dimensionality increased One original solution corresponds to many new solutions Since ∣x∣|x|∣x∣ is a convex function while −∣x∣-|x|−∣x∣ is a concave function, maximize c∣x∣c|x|c∣x∣ can only be solved when c is negative, or the solution lies on the infinite. Lecture 3: Geometry of LP Terminologies Baseline model: Min cTxs.t. Ax=bx≥0\\begin{aligned} Min \\;\\;\\bold{c^Tx} &amp;\\\\ s.t. \\;\\;\\;\\;\\bold{Ax=b} &amp;\\\\ \\bold{x}\\geq0 &amp; \\end{aligned} MincTxs.t.Ax=bx≥0​​ Feasible domain P={x∈Rn∣Ax=b,x≥0}P=\\{\\bold{x\\in R^n|Ax=b,x\\geq0}\\} P={x∈Rn∣Ax=b,x≥0} Feasible solution If x∈P\\bold{x}\\in Px∈P, then x is a feasible solution. Consistency When P≠ϕP\\neq\\phiP​=ϕ, LP is consistent. Background knowledge Definition of hyperplane Each equality constraint in the standard form LP is a “hyperplane” in the solution space. In the 2-D space, it is a line. In 3-D space, it is a plane. For a vector a∈Rn,a≠0\\bold{a}\\in\\bold{R}^n, a\\neq0a∈Rn,a​=0 and a scaler β∈R\\beta\\in\\bold{R}β∈R H={x∈Rn∣aTx=β}H=\\{\\bold{x}\\in\\bold{R}^n|\\bold{a}^T\\bold{x}=\\beta\\} H={x∈Rn∣aTx=β} is defined as a hyperplane, which divides the solution space into 3 parts, the open upper-half space Hui\\boldsymbol{H}_u^iHui​ (in which aTx&gt;β\\bold{a}^T\\bold{x}\\gt\\betaaTx&gt;β), the hyperplane and the open lower-half space Hli\\boldsymbol{H}_l^iHli​ (in which aTx&lt;β\\bold{a}^T\\bold{x}&lt;\\betaaTx&lt;β). The upper-half space Hu=Hui+H\\boldsymbol{H}_u=\\boldsymbol{H}_u^i+\\boldsymbol{H}Hu​=Hui​+H, similarly for Hl\\boldsymbol{H}_lHl​. H\\boldsymbol{H}H is also called the bounded hyperplane of Hui\\boldsymbol{H}_u^iHui​ and Hli\\boldsymbol{H}_l^iHli​. The vector a is the normal of the hyperplane and it points to the upper-half in the direction which increases $(in which aTx\\bold{a}^T\\bold{x}aTx fastest. Properties of hyperplanes Property 1: The normal vector a is orthogonal to all vectors in the hyperplane H. Property 2: The normal vector is directed toward the upper half space. Properties of feasible solution set Property 3: The feasible domain of a standard form LP P={x∈Rn∣Ax=b,x≥0}P=\\{\\bold{x\\in R^n|Ax=b,x\\geq0}\\} P={x∈Rn∣Ax=b,x≥0} is a polyhedral set. A polyhedral set or polyhedron is a set formed by the intersection of a finite number of closed half spaces. If it is nonempty or bounded, it is a polytope. Properties of optimal solutions Property 4:if P≠0P\\neq0P​=0 and ∃β∈R\\exist\\beta\\in\\bold{R}∃β∈R such that P⊂HL:={x∈Rn∣−cTx≤β}P\\subset H_L:=\\{\\bold{x\\in R^n|-c^Tx\\leq\\beta}\\} P⊂HL​:={x∈Rn∣−cTx≤β} then min⁡x∈PcTx≥−β\\min\\limits_{x\\in P}\\bold{c^Tx\\geq-\\beta}x∈Pmin​cTx≥−β. Moreover, if x∗∈P∩H\\bold{x^*}\\in P\\cap Hx∗∈P∩H, then x∗∈P∗\\bold{x^*}\\in P^*x∗∈P∗ Graphic method Step 1: Draw the feasible domain PPP Step 2: Use −c-\\bold{c}−c as normal vector at each vertex to see if P⊂HL:={x∈Rn∣−cTx≤β}P\\subset H_L:=\\{\\bold{x\\in R^n|-c^Tx\\leq\\beta}\\}P⊂HL​:={x∈Rn∣−cTx≤β} for some β∈R\\beta\\in \\bold{R}β∈R. If yes, then we found the optimal solution. Advantage: Intuitional / Geometrically simple Disadvantage: Impractical for large LP problems / Algebraically difficult Fundamental theorem of LP Background knowledge Definition: Let x1,x2,⋯ ,xp∈Rn, λ1,λ2,⋯ ,λp∈Rn\\bold{x^1,x^2,\\dotsb,x^p\\in R^n,\\; \\lambda_1,\\lambda_2,\\dotsb,\\lambda_p\\in R^n}x1,x2,⋯,xp∈Rn,λ1​,λ2​,⋯,λp​∈Rn, for x=∑i=1pλixi\\bold{x}=\\sum_{i=1}^{p}\\bold{\\lambda_ix^i} x=i=1∑p​λi​xi we say x is a linear combination of {x1,⋯ ,xp}\\{\\bold{x^1,\\dotsb,x^p}\\}{x1,⋯,xp}. If ∑i=1pλi=1\\sum_{i=1}^{p}\\lambda_i=1∑i=1p​λi​=1, w say x is an affine combination of {x1,⋯ ,xp}\\{\\bold{x^1,\\dotsb,x^p}\\}{x1,⋯,xp}. If the affine combination of any two points of S falls in S, then S is an affine set. If λi≥0\\lambda_i\\geq0λi​≥0, we say x is a conic combination of {x1,⋯ ,xp}\\{\\bold{x^1,\\dotsb,x^p}\\}{x1,⋯,xp}. If λx∈S\\lambda\\bold{x}\\in Sλx∈S for all x∈S\\bold{x}\\in Sx∈S and λ≥0\\lambda\\geq0λ≥0, then S is a cone. If ∑i=1pλi=1, λi≥0\\sum_{i=1}^{p}\\lambda_i=1,\\;\\lambda_i\\geq0∑i=1p​λi​=1,λi​≥0, w say x is an convex combination of {x1,⋯ ,xp}\\{\\bold{x^1,\\dotsb,x^p}\\}{x1,⋯,xp}. If the convex combination of ant two points of S falls in S, then S is a convex set. The geometric meaning of the feasible domain P={x∈Rn∣Ax=b,x≥0}P=\\{\\bold{x\\in R^n|Ax=b,x\\geq0}\\} P={x∈Rn∣Ax=b,x≥0} P is a polyhedral set. P is a convex set P is the intersection of m hyperplanes and the cone of the first orthant. Seeing Ax=b\\bold{Ax=b}Ax=b in the column picture (see MIT 1806), if the vector b falls in the cone generated by the columns of constraint matrix A. Interior and boundary points Definition: Given a set S∈RnS\\in\\bold{R^n}S∈Rn, a point x∈S\\bold{x\\in S}x∈S is an interior point of S if ∃ ϵ&gt;0 such that the ball B={y∈Rn∣ ∣∣x−y∣∣≤ϵ}⊂S\\exist\\;\\epsilon&gt;0\\;\\text{such that the ball}\\;\\bold{B=\\{y\\in R^n|\\;||x-y||\\leq\\epsilon\\}\\subset S} ∃ϵ&gt;0such that the ballB={y∈Rn∣∣∣x−y∣∣≤ϵ}⊂S called x∈int(S)\\bold{x\\in}int(S)x∈int(S) Otherwise, x is a boundary point of S, we say x∈bdry(S)\\bold{x}\\in bdry(S)x∈bdry(S). How to define a convex set S For all interior point, the segment between any two interior points falls in the convex set S. For ∀\\forall∀ boundary point x, ∃\\exist∃ a hyperplane H that x∈S\\bold{x\\in S}x∈S and S falls in HL or HU\\bold{H_L}\\;\\text{or}\\;\\bold{H_U}HL​orHU​. For ∀\\forall∀ outside point x, ∃\\exist∃ a hyperplane H that x lies in HL\\bold{H_L}HL​ and S falls in the HU\\bold{H_U}HU​, or vise versa. Difference among boundary points x is defined as an extreme point of a convex set S if x cannot be expressed as a convex combination of other points in S. The extreme points are not exactly the same as the vertices according to their definition. But for the feasible domain P of an LP, it vertices are the extreme points. Finding extreme points Theorem: A point x∈P={x∈Rn∣Ax=b,x≥0}\\bold{x\\in}P=\\{\\bold{x\\in R^n|Ax=b,x\\geq0}\\}x∈P={x∈Rn∣Ax=b,x≥0} is an extreme point of P if and only if the columns of A corresponding to the positive components of x are linearly independent. Proof: Without loss of generality, we may assume that the first p components of x are positive and the rest are zero, i.e., x=(xˉ0) where xˉ=(x1⋮xp) &gt;0\\bold{x=\\left(\\begin{array}{c} \\bar{x}\\\\0 \\end{array}\\right)} \\;\\text{where}\\; \\bold{\\bar{x}}=\\left(\\begin{array}{c} x_1\\\\\\vdots\\\\x_p \\end{array}\\right)\\;&gt;0 x=(xˉ0​)wherexˉ=⎝⎜⎛​x1​⋮xp​​⎠⎟⎞​&gt;0 also denote the first p column of A by Aˉ\\bold{\\bar{A}}Aˉ then Ax=Aˉxˉ=b\\bold{Ax=\\bar{A}\\bar{x}=b}Ax=Aˉxˉ=b. Suppose that the columns of Aˉ\\bold{\\bar{A}}Aˉ are not linearly independent, then ∃ wˉ≠0\\bold{\\exist\\;\\bar{w}\\neq0}∃wˉ​=0 such that Aˉwˉ=0\\bold{\\bar{A}\\bar{w}=0}Aˉwˉ=0. Notice that for ϵ\\epsilonϵ is small enough xˉ±ϵwˉ and Aˉ(xˉ±ϵwˉ)=Aˉxˉ=b\\bold{\\bar{x}\\plusmn\\epsilon\\bar{w}\\;\\text{and}\\;\\bar{A}(\\bar{x}\\plusmn\\epsilon\\bar{w})=\\bar{A}\\bar{x}=b} xˉ±ϵwˉandAˉ(xˉ±ϵwˉ)=Aˉxˉ=b Hence, y1=(xˉ+ϵwˉ0)∈P\\bold{y_1=\\left(\\begin{array}{c} \\bar{x}+\\epsilon\\bar{w}\\\\0 \\end{array}\\right)\\in \\textit{P}} y1​=(xˉ+ϵwˉ0​)∈P y2=(xˉ−ϵwˉ0)∈P\\bold{y_2=\\left(\\begin{array}{c} \\bar{x}-\\epsilon\\bar{w}\\\\0 \\end{array}\\right)\\in \\textit{P}} y2​=(xˉ−ϵwˉ0​)∈P and x=12y1+12y2\\bold{x}=\\frac{1}{2}y_1+\\frac{1}{2}y_2x=21​y1​+21​y2​, i.e. x cannot be a vertex (extreme point) of P. Thus, x is an extreme point when the columns of Aˉ\\bold{\\bar{A}}Aˉ are linearly indeoendent. Suppose that x is not an extreme point, then x=λy1+λy2\\bold{x=\\lambda y_1+\\lambda y_2}x=λy1​+λy2​ for some y1, y2∈P, y1≠y2 and 0&lt;λ&lt;1\\bold{y_1,\\;y_2\\in P,\\; y_1\\neq y_2\\;\\text{and}\\;}0&lt;\\lambda&lt;1y1​,y2​∈P,y1​​=y2​and0&lt;λ&lt;1. Since y1≥0, y2≥0\\bold{y_1\\geq0,\\;y_2\\geq0}y1​≥0,y2​≥0 and 0&lt;λ&lt;10&lt;\\lambda&lt;10&lt;λ&lt;1, the last n−pn-pn−p components of y1\\bold{y_1}y1​ must be zero, i.e. y1=(yˉ0)\\bold{y_1=\\left(\\begin{array}{c} \\bar{\\bold{y}}\\\\0 \\end{array}\\right)} y1​=(yˉ​0​) Now x−y1=(xˉ−yˉ10)≠0\\bold{x-y_1=\\left(\\begin{array}{c} \\bar{\\bold{x}}-\\bar{\\bold{y}}_1\\\\0 \\end{array}\\right)\\neq0} x−y1​=(xˉ−yˉ​1​0​)​=0 and A(x−yˉ1)=Ax−Ayˉ1=b−b=0\\bold{A(x-\\bold{\\bar{y}}_1)=Ax-A\\bar{\\bold{y}}_1=b-b=0}A(x−yˉ​1​)=Ax−Ayˉ​1​=b−b=0, which indicates that the columns of A are linearly dependent. Thus, the theorem is proofed. Managing extreme points algebraically Full rank matrix: Let A be an m by n matrix with m≤nm\\leq nm≤n, we say A has full rank (since m≤nm\\leq nm≤n, full row rank) if A has m linearly independent columns. Rearrange x as x=(xBxN)−basic variabls−non-basic variables A=(B∣ N)↑ ↑basis non-basis\\bold{ x=\\left(\\begin{array}{c} \\bold{x_B}\\\\\\bold{x_N} \\end{array}\\right) \\begin{array}{c} - &amp; \\text{basic variabls}\\\\ - &amp; \\text{non-basic variables} \\end{array} }\\;\\;\\;\\; \\begin{aligned} \\bold{A}= (&amp;\\bold{B}&amp;|\\text{\\;\\;\\;\\;}&amp;\\bold{N}&amp;)\\\\ &amp;\\uparrow&amp;\\text{\\;\\;\\;\\;}&amp; \\uparrow&amp;\\\\ &amp;\\text{basis}&amp;\\text{\\;\\;\\;\\;}&amp;\\text{non-basis}&amp; \\end{aligned} x=(xB​xN​​)−−​basic variablsnon-basic variables​A=(​B↑basis​∣​N↑non-basis​)​ If we set xN=0\\bold{x_N}=0xN​=0 and solve xB\\bold{x_B}xB​ for Ax=BxB=b\\bold{Ax=Bx_B=b}Ax=BxB​=b then x is a basic solution‾\\underline{basic\\ solution}basic solution​ (bs). Furthermore, if xB≥0\\bold{x_B}\\geq0xB​≥0, then x is a basic feasible solution‾\\underline{basic\\ feasible\\ solution}basic feasible solution​. We can see, when A does not have full rank, then either Ax = b has no solution‾\\underline{no\\ solution}no solution​ and hence P=0P=0P=0, or some constraints are redundant (which can be avoided by preprocessing matrix A). Thus, A point x in P is an extreme point of P if and only if x is a basic feasible solution corresponding to some basis B. The polyhedron P has only a finite number of extreme points (less than CmnC_m^nCmn​). What do extreme points bring us? When P={x∈Rn∣Ax=b,x≥0}P=\\{\\bold{x\\in R^n|Ax=b,x\\geq0}\\}P={x∈Rn∣Ax=b,x≥0} is a nonempty polytope, then any point in P can be represented as a convex combination of the extreme points of P. If P is unbounded, there exists an extremal direction. Definition: A vector d(≠0)∈Rn\\bold{d(\\neq0)\\in R^n}d(​=0)∈Rn is an extremal direction of P, if {x∈Rn∣x=x0+λd, λ≥0}⊂P\\{\\bold{x\\in R^n|x=x_0+\\lambda d,\\ \\lambda\\geq0\\}\\subset }P{x∈Rn∣x=x0​+λd, λ≥0}⊂P for all x0∈P\\bold{x_0}\\in Px0​∈P. d satisfies Ad=0\\bold{Ad=0}Ad=0 and d≥0\\bold{d\\geq 0}d≥0. Resolution theorem Let V={vi∈Rn∣i∈I}\\bold{V = \\{v_i\\in R^n|}i\\in I\\}V={vi​∈Rn∣i∈I} be a set of extreme points of P, then for ∀x∈P\\forall x \\in P∀x∈P x=∑i∈Iλivi+d\\bold{x}=\\sum\\limits_{i\\in I} \\lambda_iv_i+\\bold{d} x=i∈I∑​λi​vi​+d where ∑i∈Iλi=1, λi≥0, ∀i∈I.\\sum\\limits_{i\\in I}\\lambda_i=1,\\ \\lambda_i\\geq0,\\ \\forall i\\in I. i∈I∑​λi​=1, λi​≥0, ∀i∈I. and either d=0\\bold{d=0}d=0 or d is an extremal direction of P Fundamental theorem of LP For a standard form LP, if its feasible domain P is nonempty, then the optimal objective value of z=cTx over P\\bold{z=c^Tx}\\ over\\ Pz=cTx over P is either unbounded below, or it is attained at (at least) an extreme point of P. Lecture 4: Simplex Method Basic Idea: Phase 1: Step 1 (Starting): Find an initial extreme point (ep) or declare P in null. Phase 2: Step 2 (Checking optimality): if the current eo is optimal, STOP. Step 3 (Pivoting): Move to a better ep, then return to Step 2. If we do not repeat using the same extreme point, then the algorithm will always terminates in finite number of iterations. But how to efficiently generate better extreme points? In the algebra term, we just need to use basic feasible solution to replace extreme point above. If there exists at least one basic variable becoming zero, the extreme point may correspond to more than one basic feasible solution, in this case we say the bfs is a degenerate one. Nondegeneracy Property 1: If a bfs x is nondegenerate, then x is uniquely determined by n hyperplanes. Let M=[BN0I]\\bold{M=}\\left[\\begin{array}{cc} \\bold{B} &amp; \\bold{N}\\\\ \\bold0 &amp; \\bold{I} \\end{array}\\right]M=[B0​NI​], M is a nonsingular matrix. x is uniquely determined by n linearly independent hyperplanes since Mx=[BN0I][xBxN]=[b0]\\bold{Mx}=\\left[\\begin{array}{cc} \\bold{B} &amp; \\bold{N}\\\\ \\bold{0} &amp; \\bold{I} \\end{array}\\right]\\left[\\begin{array}{c} \\bold{x_B}\\\\ \\bold{x_N} \\end{array}\\right]=\\left[\\begin{array}{c} \\bold{b}\\\\ \\bold{0} \\end{array}\\right] Mx=[B0​NI​][xB​xN​​]=[b0​] x=[xBxN]=M−1[b0]\\bold{x}=\\left[\\begin{array}{c} \\bold{x_B}\\\\\\bold{x_N} \\end{array}\\right]=\\bold{M^{-1}}\\left[\\begin{array}{c} \\bold{b}\\\\ \\bold{0} \\end{array}\\right] x=[xB​xN​​]=M−1[b0​] in which, M−1=[B−1−B−1N0I]\\bold{M^{-1}}=\\left[\\begin{array}{cc} \\bold{B^{-1}} &amp; \\bold{-B^{-1}N}\\\\ \\bold{0} &amp; \\bold{I} \\end{array}\\right] M−1=[B−10​−B−1NI​] We call M−1\\bold{M^{-1}}M−1 (or M) the fundamental matrix of LP. Property 2: If a bfs x is degenerate, then x is overdetermined by more than n hyperplanes. Other than n hyperplanes determined by non-basic variables, there exists at least one basic variable that xi=0x_i=0xi​=0, which indicates an extra hyperplane. Simplex method under nondegeneracy Basic idea: Instead of considering all bfs at the same time, wo just consider some neighboring bfs, moving from one bfs to another with a simple pivoting scheme. Definition Two basic feasible solution are adjacent if they have m-1\\textit{m-1}m-1 basic variables in common. Each bfs has m-n\\textit{m-n}m-n adjacent neighbors, which can be reached by increasing one non basic variable from zero to positive and decrease one basic variable to 0, called pivoting."},{"title":"LA01: The geometry of linear equations","date":"2020-05-27T03:06:22.000Z","updated":"2020-05-29T07:25:55.258Z","comments":true,"path":"Notes/LinearAlgebra/LA01-the-geometry-of-linear-equations/index.html","permalink":"http://yoursite.com/Notes/LinearAlgebra/LA01-the-geometry-of-linear-equations/index.html","excerpt":"","text":"The fundamental problem of linear algebra is to solve n linear equations in n unknowns, for example: 2x−y = 0−x+2y = 3\\begin{aligned} 2x-y\\,=\\,0\\\\ -x+2y\\,=\\,3 \\end{aligned} 2x−y=0−x+2y=3​ In the first lecture, Dr. Strang show us three ways to view this problem. Row Picture Plot the points that satisfied each equation. The intersection of the plots (if they do intersect) is the solution of the system of above equations, which is x = 1,y = 2x\\,=\\,1, y\\,=\\,2x=1,y=2. Figure 1: The plots of the above equations intersect at the point (1,2) We substitute (1,2) into the original system of equations to check it’s validity: 2⋅1−2 = 0−1+2⋅2 = 3\\begin{aligned} 2\\cdot1-2\\,=\\,0\\\\ -1+2\\cdot2\\,=\\,3 \\end{aligned} 2⋅1−2=0−1+2⋅2=3​ Similarly, the solution of a three dimensional system is the common intersection of those three planes (if there does exist one). Column Picture In the column picture, we rewrite the system as a single equation by turning the coefficients in the column of the system into vectors: x[21]+y[−12]=[03]\\begin{aligned} x \\left[ \\begin{array}{c} 2\\\\ 1 \\end{array} \\right] +y \\left[ \\begin{array}{c} -1\\\\ 2 \\end{array} \\right] = \\left[ \\begin{array}{c} 0\\\\ 3 \\end{array} \\right] \\end{aligned} x[21​]+y[−12​]=[03​]​ Given two vectors c and d and scalars x and y, the sum xc+yd is called a linear combination of c and d, which is an important concept throughout Linear Algebra. Geometrically, we are looking for a pair of x and y which satisfies that x copies of vector [2−1]\\left[\\begin{array}{c}2\\\\-1\\end{array}\\right][2−1​] added to y copies of vector [−12]\\left[\\begin{array}{c}-1\\\\2\\end{array}\\right][−12​] equals the vector [03]\\left[\\begin{array}{c}0\\\\3\\end{array}\\right][03​]. As shown in Figure 2, x=1 and y=2x=1 \\;\\text{and}\\; y=2x=1andy=2 agreeing with the result we got from row picture. Figure 2: A linear combination of the column vectors. In the three dimensions, the column picture requires to find a linear combination of 3-dimensional vectors that equals to the vector b. Matrix Picture Rewrite the system of equations as a single equation by using matrices and vectors: [2−1−12][xy]=[03]\\begin{aligned} \\left[ \\begin{array}{cc} 2 &amp; -1\\\\ -1 &amp; 2\\\\ \\end{array} \\right] \\left[ \\begin{array}{c} x\\\\ y \\end{array} \\right] =\\left[ \\begin{array}{c} 0\\\\ 3 \\end{array} \\right] \\end{aligned} [2−1​−12​][xy​]=[03​]​ The matrix [2−1−12]\\left[\\begin{array}{cc}2&amp;-1\\\\-1&amp;2\\end{array}\\right][2−1​−12​] is called the coefficient matrix. The vector x=[xy]\\bold{x}=\\left[\\begin{array}{c}x\\\\y\\end{array}\\right]x=[xy​] is the vector of unknowns. The value on the right hand side of the equations form the vector b: Ax=bA\\bold{x}=\\bold{b} Ax=b The three dimensional matrix picture is similar to the two dimensional one, except that the vectors and matrices increase in size. Matrix Multiplication How do we multiply a matrix A by a vector x? [2513][12]= ?\\left[ \\begin{array}{cc} 2&amp;5\\\\ 1&amp;3 \\end{array} \\right] \\left[ \\begin{array}{c} 1\\\\2 \\end{array} \\right] =\\;? [21​53​][12​]=? The method Dr. Strang suggests is to think of the entries of x as the coefficients of a linear combination of the column vectors of the matrix: [2513][12]=1[21]+2[53]=[127]\\left[ \\begin{array}{cc} 2&amp;5\\\\1&amp;3 \\end{array} \\right] \\left[ \\begin{array}{c} 1\\\\2 \\end{array} \\right] =1\\left[ \\begin{array}{c} 2\\\\1 \\end{array} \\right] +2\\left[ \\begin{array}{c} 5\\\\3 \\end{array} \\right] =\\left[ \\begin{array}{c} 12\\\\7 \\end{array} \\right] [21​53​][12​]=1[21​]+2[53​]=[127​] The technique shows that Ax is a linear combination of the columns of A. Also, you can calculate the product Ax by taking dot product of each row of A with the vector x: [2513][12]=[2⋅1+5⋅21⋅1+3⋅2]=[127]\\left[ \\begin{array}{cc} 2&amp;5\\\\1&amp;3 \\end{array} \\right] \\left[ \\begin{array}{c} 1\\\\2 \\end{array} \\right] =\\left[ \\begin{array}{c} 2\\cdot1+5\\cdot2\\\\1\\cdot1+3\\cdot2 \\end{array} \\right] =\\left[ \\begin{array}{c} 12\\\\7 \\end{array} \\right] [21​53​][12​]=[2⋅1+5⋅21⋅1+3⋅2​]=[127​] Linear Independence In the column and matrix pictures, the right hand side of the equation is a vector b. Given a matrix A, if we can solve: Ax=bA\\bold{x}=\\bold{b} Ax=b for every possible b, we say that A is an inversible matrix, which means that the linear combinations of the column vectors fill the xy-plane (in two dimensional case). Otherwise, we say that A is a singular matrix, whose column vectors are linear dependent, or in other words, all linear combinations of those vectors lie on a point or line (in two dimensional case). In such case, the combinations don’t fill the whole space."}],"posts":[{"title":"Homepage","slug":"Homepage","date":"2020-05-15T02:43:53.000Z","updated":"2020-06-29T06:47:27.909Z","comments":true,"path":"2020/05/15/Homepage/","link":"","permalink":"http://yoursite.com/2020/05/15/Homepage/","excerpt":"","text":"Welcome What’s New June 14 2020, An end is also a start I have graduated from the school of Electrical and Electronic Engineering, Huazhong University of Science and Technology. At the same time, I am awarded the Bachelor’s Honors Degree by SEEE, HUST. From September 2020, I will be a PhD student in the University of Macau, supervised by Prof. Hongcai Zhang. As a candidate of UM “1+3” PhD programme, I am going to spend the first year of my PhD study in Imperial College London, majoring in Future Power Networks. May 2020, A really useful book: I am reading the book “Crafting Your Research Future: A Guide to Successful Master’s and Ph.D. Degrees in Science &amp; Engineering” these days. It should be beneficial to all Ph.D. candidates who wants to have a clear picture of their future. May 2020, New column! I am going to update my study notes about linear algebra and convex optimization to a new column “Notes”. My plan is to follow MIT 1806 (Linear Algebra) in May for review and start cvx in June. Feiyue handbook for ECE students is now available! See Feiyue for ECE page for more. 《华中大电气飞跃手册》是一本面向电气学子的申请指南，总结了编委会成员的留学申请经验，涵盖关于留学申请的常见问题，包含道路选择、托福雅思GRE考试、科研暑研、套瓷、签证等内容。意图帮助大家对留学申请全过程建立起比较清晰、全面的认识。除此之外，编委会还整理了学校海外交流项目政策并汇编了电气学院往届学长学姐的留学申请经历，帮助同学们更好地明确自己的定位，更加合理地进行学校选择和申请准备。 My website is available now. I will update my research progress and works here. Related Links Prof. Hongcai Zhang Prof. Yonghua Song State Key Laboratory of Internet of Things for Smart City","categories":[],"tags":[]}],"categories":[],"tags":[]}